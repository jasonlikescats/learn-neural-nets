{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonlikescats/learn-neural-nets/blob/colab/text-gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "af05478e-92e5-49a0-9e9d-389327f1d4a3",
      "metadata": {
        "id": "af05478e-92e5-49a0-9e9d-389327f1d4a3"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from graphviz import Digraph\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "75d11b33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75d11b33",
        "outputId": "f59e90e7-3db2-4f07-a24f-a7c48d0f80d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn', 'abigail', 'emily', 'elizabeth', 'mila', 'ella', 'avery', 'sofia', 'camila', 'aria', 'scarlett']\n"
          ]
        }
      ],
      "source": [
        "# read names.txt\n",
        "with open('names.txt') as f:\n",
        "    names = f.read().splitlines()\n",
        "\n",
        "print(names[:20])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# abstract base\n",
        "class NGramModel(ABC):\n",
        "    delimiter_token = \".\"\n",
        "    vocab = [delimiter_token] + list(string.ascii_lowercase)\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self, words):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @abstractmethod\n",
        "    def loss(self, words):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _inputs(self, words, pad_count = 1):\n",
        "        for w in words:\n",
        "            encoded = self._encode(w, pad_count)\n",
        "            for ngrams in self._ngrams(encoded):\n",
        "                yield ngrams\n",
        "\n",
        "    def _encode(self, word, pad_count = 1):\n",
        "        # pad the word with `pad_count` start tokens, and a trailing end token\n",
        "        delim_token = self.__class__.delimiter_token\n",
        "        padded_word = delim_token * pad_count + word + delim_token\n",
        "        encoded = [self.vocab.index(c) for c in padded_word]\n",
        "        return encoded\n",
        "\n",
        "    def _ngrams(self, encoded_word):\n",
        "        for i in range(0, len(encoded_word) - self.n + 1):\n",
        "            yield encoded_word[i:i + self.n]\n",
        "\n",
        "    def _decode(self, encoded):\n",
        "        return \"\".join([self.vocab[i] for i in encoded])\n"
      ],
      "metadata": {
        "id": "x2MnEOBCmCQh"
      },
      "id": "x2MnEOBCmCQh",
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "8152f10f",
      "metadata": {
        "id": "8152f10f"
      },
      "outputs": [],
      "source": [
        "class NGramCountingModel(NGramModel):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "        self.encode_pad_count = self.n - 1\n",
        "        self.counts = torch.ones((len(self.vocab),) * n, dtype=torch.int32) # start at ones to apply some smoothing\n",
        "\n",
        "    def train(self, words):\n",
        "        for input_ngrams in self._inputs(words, pad_count = self.encode_pad_count):\n",
        "            self.counts[tuple(input_ngrams)] += 1\n",
        "\n",
        "        self._normalize()\n",
        "\n",
        "    def loss(self, words):\n",
        "        # calculate negative log likelihood loss\n",
        "        log_likelihood = 0.0\n",
        "        n = 0\n",
        "\n",
        "        for w in words:\n",
        "            encoded = self._encode(w, pad_count = self.encode_pad_count)\n",
        "            for ngrams in self._ngrams(encoded):\n",
        "                prob = self.counts[tuple(ngrams)]\n",
        "                logprob = torch.log(prob)\n",
        "                log_likelihood += logprob\n",
        "                n += 1\n",
        "\n",
        "        nll = -log_likelihood\n",
        "        return nll / n\n",
        "\n",
        "    def predict(self):\n",
        "        # start with n-1 start tokens\n",
        "        prefix = [0] * self.encode_pad_count\n",
        "\n",
        "        # generate a word\n",
        "        word = []\n",
        "        while True:\n",
        "            # get the next token\n",
        "            token = torch.multinomial(self.counts[tuple(prefix)], 1).item()\n",
        "            word.append(token)\n",
        "            prefix = prefix[1:] + [token]\n",
        "            if token == 0:\n",
        "                break\n",
        "        return self._decode(word)\n",
        "\n",
        "    def _normalize(self):\n",
        "        self.counts = self.counts / self.counts.sum(dim=-1, keepdim=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramNeuralNetModel(NGramModel):\n",
        "    def __init__(self, steps, learning_rate, regularization_strength):\n",
        "        super().__init__()\n",
        "        # randomly initialize 27 neurons' weights. Each neuron receives 27 inputs.\n",
        "        self.W = torch.randn((len(self.vocab), len(self.vocab)), requires_grad=True)\n",
        "        self.steps = steps\n",
        "        self.lr = learning_rate\n",
        "        self.rs = regularization_strength\n",
        "\n",
        "    def train(self, words):\n",
        "        xenc, yenc = self._one_hot_inputs(words) # input to the network: one-hot encoding\n",
        "\n",
        "        # gradient descent\n",
        "        for k in range(self.steps):\n",
        "            logits = self._forward_pass(xenc)\n",
        "            probs = self._softmax(logits)\n",
        "            loss = self._loss(probs, yenc)\n",
        "\n",
        "            # backward pass\n",
        "            self.W.grad = None\n",
        "            loss.backward()\n",
        "\n",
        "            # update weights\n",
        "            self.W.data += -self.lr * self.W.grad\n",
        "\n",
        "    def loss(self, words):\n",
        "        xenc, yenc = self._one_hot_inputs(words)\n",
        "        probs = self._forward_pass(xenc)\n",
        "        probs = self._softmax(probs)\n",
        "        return self._loss(probs, yenc)\n",
        "\n",
        "    def predict(self):\n",
        "        out = []\n",
        "        ix = 0\n",
        "        while True:\n",
        "            xenc = F.one_hot(torch.tensor([ix]), num_classes=len(self.vocab)).float()\n",
        "            logits = self._forward_pass(xenc)\n",
        "            probs = self._softmax(logits)\n",
        "\n",
        "            ix = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
        "            out.append(self.vocab[ix])\n",
        "            if ix == 0:\n",
        "                break\n",
        "\n",
        "        return \"\".join(out)\n",
        "\n",
        "    def _one_hot_inputs(self, words):\n",
        "        xs, ys = [], []\n",
        "        for w in words:\n",
        "            encoded = self._encode(w, pad_count=1)\n",
        "            xs.extend(encoded[:-1])\n",
        "            ys.extend(encoded[1:])\n",
        "\n",
        "        xs = torch.tensor(xs)\n",
        "        ys = torch.tensor(ys)\n",
        "\n",
        "        xenc = F.one_hot(xs, num_classes=len(self.vocab)).float()\n",
        "        yenc = F.one_hot(ys, num_classes=len(self.vocab)).float()\n",
        "\n",
        "        return xenc, yenc\n",
        "\n",
        "    def _loss(self, probs, yactual):\n",
        "        masked = probs * yactual\n",
        "        likelihood = masked.sum(dim=1)\n",
        "        log_likelihood = torch.log(likelihood)\n",
        "        nll = -log_likelihood\n",
        "\n",
        "        regularization_loss = (self.W ** 2).mean()\n",
        "\n",
        "        return nll.mean() + self.rs * regularization_loss\n",
        "\n",
        "    def _forward_pass(self, xenc):\n",
        "        logits = xenc @ self.W # predict log-counts\n",
        "        return logits\n",
        "\n",
        "    def _softmax(self, logits):\n",
        "        counts = logits.exp() # equivalent to counts array (N) in counts model\n",
        "        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "        return probs\n",
        "\n",
        "    def _backward_pass(self, loss):\n",
        "        self.W.grad = None # zero the gradient\n",
        "        loss.backward()"
      ],
      "metadata": {
        "id": "R5Vu342DmAW6"
      },
      "id": "R5Vu342DmAW6",
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use a subset of the names as the training set\n",
        "holdout = 0.2\n",
        "training_set = names[:int(len(names) * (1.0 - holdout))]\n",
        "testing_set = names[int(len(names) * (1.0 - holdout)):]"
      ],
      "metadata": {
        "id": "piOza1yPGF3a"
      },
      "id": "piOza1yPGF3a",
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "id": "6c88e28d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c88e28d",
        "outputId": "b761ab37-4c68-4e0d-a467-50c119f6d076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram loss (training set): 2.42488956451416\n",
            "bigram loss (testing set): 2.593641757965088\n",
            "trigram loss (training set): 2.1762611865997314\n",
            "trigram loss (testing set): 2.4267663955688477\n",
            "nn bigram loss (training set): 2.569892644882202\n",
            "nn bigram loss (testing set): 2.728921890258789\n",
            "nn bigram (no reg) loss (training set): 2.4678573608398438\n",
            "nn bigram (no reg) loss (testing set): 2.6361002922058105\n"
          ]
        }
      ],
      "source": [
        "bigram = NGramCountingModel(2)\n",
        "bigram.train(training_set)\n",
        "print(f\"bigram loss (training set): {bigram.loss(training_set)}\")\n",
        "print(f\"bigram loss (testing set): {bigram.loss(testing_set)}\")\n",
        "\n",
        "trigram = NGramCountingModel(3)\n",
        "trigram.train(training_set)\n",
        "print(f\"trigram loss (training set): {trigram.loss(training_set)}\")\n",
        "print(f\"trigram loss (testing set): {trigram.loss(testing_set)}\")\n",
        "\n",
        "nn_bigram = BigramNeuralNetModel(steps=50, learning_rate=50, regularization_strength=0.1)\n",
        "nn_bigram.train(training_set)\n",
        "print(f\"nn bigram loss (training set): {nn_bigram.loss(training_set)}\")\n",
        "print(f\"nn bigram loss (testing set): {nn_bigram.loss(testing_set)}\")\n",
        "\n",
        "nn_bigram_no_reg = BigramNeuralNetModel(steps=50, learning_rate=50, regularization_strength=0.00)\n",
        "nn_bigram_no_reg.train(training_set)\n",
        "print(f\"nn bigram (no reg) loss (training set): {nn_bigram_no_reg.loss(training_set)}\")\n",
        "print(f\"nn bigram (no reg) loss (testing set): {nn_bigram_no_reg.loss(testing_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "id": "8c1755e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c1755e0",
        "outputId": "08f82077-3a24-40cc-c60f-16576d9d4d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BIGRAM\n",
            "ya.\n",
            "syahle.\n",
            "ahe.\n",
            "dleekahmangonya.\n",
            "tryahe.\n",
            "chen.\n",
            "ena.\n",
            "dlyamiiae.\n",
            "a.\n",
            "keles.\n",
            "\n",
            "TRIGRAM\n",
            "lo.\n",
            "atoy.\n",
            "alityn.\n",
            "pepolannezika.\n",
            "shallami.\n",
            "ton.\n",
            "clen.\n",
            "camarivie.\n",
            "augrten.\n",
            "kirahmolwqei.\n",
            "\n",
            "NN BIGRAM\n",
            "ionylyanychaizdah.\n",
            "ane.\n",
            "bpniyann.\n",
            "jskako.\n",
            "h.\n",
            "eyamilyi.\n",
            "zailanaeleenoxtelygylysqra.\n",
            "nisema.\n",
            "malahe.\n",
            "ktorpaclilyai.\n",
            "\n",
            "NN BIGRAM (NO REGULARIZATION)\n",
            "axstlsa.\n",
            "ckennar.\n",
            "beriandeishliras.\n",
            "jalel.\n",
            "knai.\n",
            "kajaharon.\n",
            "da.\n",
            "mirlyzaneuenayllea.\n",
            "i.\n",
            "marese.\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"BIGRAM\")\n",
        "for i in range(10):\n",
        "    print(bigram.predict())\n",
        "\n",
        "print()\n",
        "print(\"TRIGRAM\")\n",
        "for i in range(10):\n",
        "    print(trigram.predict())\n",
        "\n",
        "print()\n",
        "print(\"NN BIGRAM\")\n",
        "for i in range(10):\n",
        "    print(nn_bigram.predict())\n",
        "\n",
        "print()\n",
        "print(\"NN BIGRAM (NO REGULARIZATION)\")\n",
        "for i in range(10):\n",
        "    print(nn_bigram_no_reg.predict())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}