{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonlikescats/learn-neural-nets/blob/main/text-gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "af05478e-92e5-49a0-9e9d-389327f1d4a3",
      "metadata": {
        "id": "af05478e-92e5-49a0-9e9d-389327f1d4a3"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from graphviz import Digraph\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "75d11b33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75d11b33",
        "outputId": "28c0cafc-0ff6-43fb-e180-9fd9d8b2625f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn', 'abigail', 'emily', 'elizabeth', 'mila', 'ella', 'avery', 'sofia', 'camila', 'aria', 'scarlett']\n"
          ]
        }
      ],
      "source": [
        "# read names.txt\n",
        "with open('names.txt') as f:\n",
        "    names = f.read().splitlines()\n",
        "\n",
        "print(names[:20])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# abstract base\n",
        "class NGramModel(ABC):\n",
        "    delimiter_token = \".\"\n",
        "    vocab = [delimiter_token] + list(string.ascii_lowercase)\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self, words):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @abstractmethod\n",
        "    def loss(self, words):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _inputs(self, words, pad_count = 1):\n",
        "        for w in words:\n",
        "            encoded = self._encode(w, pad_count)\n",
        "            for ngrams in self._ngrams(encoded):\n",
        "                yield ngrams\n",
        "\n",
        "    def _encode(self, word, pad_count = 1):\n",
        "        # pad the word with `pad_count` start tokens, and a trailing end token\n",
        "        delim_token = self.__class__.delimiter_token\n",
        "        padded_word = delim_token * pad_count + word + delim_token\n",
        "        encoded = [self.vocab.index(c) for c in padded_word]\n",
        "        return encoded\n",
        "\n",
        "    def _ngrams(self, encoded_word):\n",
        "        for i in range(0, len(encoded_word) - self.n + 1):\n",
        "            yield encoded_word[i:i + self.n]\n",
        "\n",
        "    def _decode(self, encoded):\n",
        "        return \"\".join([self.vocab[i] for i in encoded])\n"
      ],
      "metadata": {
        "id": "x2MnEOBCmCQh"
      },
      "id": "x2MnEOBCmCQh",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8152f10f",
      "metadata": {
        "id": "8152f10f"
      },
      "outputs": [],
      "source": [
        "class NGramCountingModel(NGramModel):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "        self.encode_pad_count = self.n - 1\n",
        "        self.counts = torch.ones((len(self.vocab),) * n, dtype=torch.int32) # start at ones to apply some smoothing\n",
        "\n",
        "    def train(self, words):\n",
        "        for input_ngrams in self._inputs(words, pad_count = self.encode_pad_count):\n",
        "            self.counts[tuple(input_ngrams)] += 1\n",
        "\n",
        "        self._normalize()\n",
        "\n",
        "    def loss(self, words):\n",
        "        # calculate negative log likelihood loss\n",
        "        log_likelihood = 0.0\n",
        "        n = 0\n",
        "\n",
        "        for w in words:\n",
        "            encoded = self._encode(w, pad_count = self.encode_pad_count)\n",
        "            for ngrams in self._ngrams(encoded):\n",
        "                prob = self.counts[tuple(ngrams)]\n",
        "                logprob = torch.log(prob)\n",
        "                log_likelihood += logprob\n",
        "                n += 1\n",
        "\n",
        "        nll = -log_likelihood\n",
        "        return nll / n\n",
        "\n",
        "    def predict(self):\n",
        "        # start with n-1 start tokens\n",
        "        prefix = [0] * self.encode_pad_count\n",
        "\n",
        "        # generate a word\n",
        "        word = []\n",
        "        while True:\n",
        "            # get the next token\n",
        "            token = torch.multinomial(self.counts[tuple(prefix)], 1).item()\n",
        "            word.append(token)\n",
        "            prefix = prefix[1:] + [token]\n",
        "            if token == 0:\n",
        "                break\n",
        "        return self._decode(word)\n",
        "\n",
        "    def _normalize(self):\n",
        "        self.counts = self.counts / self.counts.sum(dim=-1, keepdim=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramNeuralNetModel(NGramModel):\n",
        "    def __init__(self, steps, learning_rate, regularization_strength):\n",
        "        super().__init__()\n",
        "        # randomly initialize 27 neurons' weights. Each neuron receives 27 inputs.\n",
        "        self.W = torch.randn((len(self.vocab), len(self.vocab)), requires_grad=True)\n",
        "        self.steps = steps\n",
        "        self.lr = learning_rate\n",
        "        self.rs = regularization_strength\n",
        "\n",
        "    def train(self, words):\n",
        "        xenc, yenc = self._one_hot_inputs(words) # input to the network: one-hot encoding\n",
        "\n",
        "        # gradient descent\n",
        "        for k in range(self.steps):\n",
        "            logits = self._forward_pass(xenc)\n",
        "            probs = self._softmax(logits)\n",
        "            loss = self._loss(probs, yenc)\n",
        "\n",
        "            # backward pass\n",
        "            self.W.grad = None\n",
        "            loss.backward()\n",
        "\n",
        "            # update weights\n",
        "            self.W.data += -self.lr * self.W.grad\n",
        "\n",
        "    def loss(self, words):\n",
        "        xenc, yenc = self._one_hot_inputs(words)\n",
        "        probs = self._forward_pass(xenc)\n",
        "        probs = self._softmax(probs)\n",
        "        return self._loss(probs, yenc)\n",
        "\n",
        "    def predict(self):\n",
        "        out = []\n",
        "        ix = 0\n",
        "        while True:\n",
        "            xenc = F.one_hot(torch.tensor([ix]), num_classes=len(self.vocab)).float()\n",
        "            logits = self._forward_pass(xenc)\n",
        "            probs = self._softmax(logits)\n",
        "\n",
        "            ix = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
        "            out.append(self.vocab[ix])\n",
        "            if ix == 0:\n",
        "                break\n",
        "\n",
        "        return \"\".join(out)\n",
        "\n",
        "    def _one_hot_inputs(self, words):\n",
        "        xs, ys = [], []\n",
        "        for w in words:\n",
        "            encoded = self._encode(w, pad_count=1)\n",
        "            xs.extend(encoded[:-1])\n",
        "            ys.extend(encoded[1:])\n",
        "\n",
        "        xs = torch.tensor(xs)\n",
        "        ys = torch.tensor(ys)\n",
        "\n",
        "        xenc = F.one_hot(xs, num_classes=len(self.vocab)).float()\n",
        "        yenc = F.one_hot(ys, num_classes=len(self.vocab)).float()\n",
        "\n",
        "        return xenc, yenc\n",
        "\n",
        "    def _loss(self, probs, yactual):\n",
        "        masked = probs * yactual\n",
        "        likelihood = masked.sum(dim=1)\n",
        "        log_likelihood = torch.log(likelihood)\n",
        "        nll = -log_likelihood\n",
        "\n",
        "        regularization_loss = (self.W ** 2).mean()\n",
        "\n",
        "        return nll.mean() + self.rs * regularization_loss\n",
        "\n",
        "    def _forward_pass(self, xenc):\n",
        "        logits = xenc @ self.W # predict log-counts\n",
        "        return logits\n",
        "\n",
        "    def _softmax(self, logits):\n",
        "        counts = logits.exp() # equivalent to counts array (N) in counts model\n",
        "        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "        return probs\n",
        "\n",
        "    def _backward_pass(self, loss):\n",
        "        self.W.grad = None # zero the gradient\n",
        "        loss.backward()"
      ],
      "metadata": {
        "id": "R5Vu342DmAW6"
      },
      "id": "R5Vu342DmAW6",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use a subset of the names as the training set\n",
        "holdout = 0.2\n",
        "training_set = names[:int(len(names) * (1.0 - holdout))]\n",
        "testing_set = names[int(len(names) * (1.0 - holdout)):]"
      ],
      "metadata": {
        "id": "piOza1yPGF3a"
      },
      "id": "piOza1yPGF3a",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6c88e28d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c88e28d",
        "outputId": "2f19a1f0-96e5-4000-ed8d-0b7855a95e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram loss (training set): 2.42488956451416\n",
            "bigram loss (testing set): 2.593641757965088\n",
            "trigram loss (training set): 2.1762611865997314\n",
            "trigram loss (testing set): 2.4267663955688477\n",
            "nn bigram loss (training set): 2.569553852081299\n",
            "nn bigram loss (testing set): 2.7266845703125\n",
            "nn bigram (no reg) loss (training set): 2.4683141708374023\n",
            "nn bigram (no reg) loss (testing set): 2.6408777236938477\n"
          ]
        }
      ],
      "source": [
        "bigram = NGramCountingModel(2)\n",
        "bigram.train(training_set)\n",
        "print(f\"bigram loss (training set): {bigram.loss(training_set)}\")\n",
        "print(f\"bigram loss (testing set): {bigram.loss(testing_set)}\")\n",
        "\n",
        "trigram = NGramCountingModel(3)\n",
        "trigram.train(training_set)\n",
        "print(f\"trigram loss (training set): {trigram.loss(training_set)}\")\n",
        "print(f\"trigram loss (testing set): {trigram.loss(testing_set)}\")\n",
        "\n",
        "nn_bigram = BigramNeuralNetModel(steps=50, learning_rate=50, regularization_strength=0.1)\n",
        "nn_bigram.train(training_set)\n",
        "print(f\"nn bigram loss (training set): {nn_bigram.loss(training_set)}\")\n",
        "print(f\"nn bigram loss (testing set): {nn_bigram.loss(testing_set)}\")\n",
        "\n",
        "nn_bigram_no_reg = BigramNeuralNetModel(steps=50, learning_rate=50, regularization_strength=0.00)\n",
        "nn_bigram_no_reg.train(training_set)\n",
        "print(f\"nn bigram (no reg) loss (training set): {nn_bigram_no_reg.loss(training_set)}\")\n",
        "print(f\"nn bigram (no reg) loss (testing set): {nn_bigram_no_reg.loss(testing_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8c1755e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c1755e0",
        "outputId": "bebada5f-5014-4981-f072-cc361e8fd62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BIGRAM\n",
            "a.\n",
            "feeenvi.\n",
            "s.\n",
            "mabian.\n",
            "dan.\n",
            "stan.\n",
            "silaylelaremah.\n",
            "li.\n",
            "le.\n",
            "epiachalen.\n",
            "\n",
            "TRIGRAM\n",
            "dmzi.\n",
            "kence.\n",
            "jordon.\n",
            "kalla.\n",
            "miqrqyjaya.\n",
            "vihia.\n",
            "acen.\n",
            "kaitharcephelia.\n",
            "sotte.\n",
            "seliya.\n",
            "\n",
            "NN BIGRAM\n",
            "kgha.\n",
            "abr.\n",
            "n.\n",
            "annara.\n",
            "reynnn.\n",
            "sor.\n",
            "pjjiewx.\n",
            "liljahm.\n",
            "fhi.\n",
            "bradonele.\n",
            "\n",
            "NN BIGRAM (NO REGULARIZATION)\n",
            "ky.\n",
            "k.\n",
            "feliavaha.\n",
            "aanone.\n",
            "brral.\n",
            "cdo.\n",
            "t.\n",
            "damayanel.\n",
            "ritahjairiatla.\n",
            "ka.\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"BIGRAM\")\n",
        "for i in range(10):\n",
        "    print(bigram.predict())\n",
        "\n",
        "print()\n",
        "print(\"TRIGRAM\")\n",
        "for i in range(10):\n",
        "    print(trigram.predict())\n",
        "\n",
        "print()\n",
        "print(\"NN BIGRAM\")\n",
        "for i in range(10):\n",
        "    print(nn_bigram.predict())\n",
        "\n",
        "print()\n",
        "print(\"NN BIGRAM (NO REGULARIZATION)\")\n",
        "for i in range(10):\n",
        "    print(nn_bigram_no_reg.predict())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(\"\".join(names))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi[NGramModel.delimiter_token] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdNSsrER3i4M",
        "outputId": "2c2d55ab-8ad1-4414-a48c-b1dead2b4aad"
      },
      "id": "NdNSsrER3i4M",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(NGramModel):\n",
        "    block_size = 3\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        super().__init__()\n",
        "        self.n = self.block_size + 1\n",
        "\n",
        "    def train(self, words):\n",
        "        self._dataset(words[:5])\n",
        "\n",
        "    def loss(self, words):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def predict(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _dataset(self, words):\n",
        "        X, Y = [], []\n",
        "\n",
        "        ngrams = self._inputs(words, pad_count = self.block_size)\n",
        "        for ngram in ngrams:\n",
        "            X.append(ngram[:self.block_size])\n",
        "            Y.append(ngram[-1])\n",
        "\n",
        "        X = torch.tensor(X)\n",
        "        Y = torch.tensor(Y)\n",
        "        X, Y\n",
        "\n",
        "chars = sorted(list(set(\"\".join(names))))\n",
        "MLP(chars).train(names)"
      ],
      "metadata": {
        "id": "kL2-qtSt4BkK"
      },
      "id": "kL2-qtSt4BkK",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ywXzQ5HX53mC"
      },
      "id": "ywXzQ5HX53mC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}